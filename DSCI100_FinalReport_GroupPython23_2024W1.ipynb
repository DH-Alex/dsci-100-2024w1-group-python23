{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Load the packages\n",
    "import pandas as pd\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading the Data\n",
    "\n",
    "In this step, we load the datasets into Python using the `pandas` library to demonstrate that the dataset can be loaded into Python.\n",
    "We read the `players.csv` and `sessions.csv` files, which contain essential details about player characteristics and session activities, respectively.\n",
    "\n",
    "The code used for this step accomplishes the following:\n",
    "- It imports the data from the relative file paths into pandas DataFrames.\n",
    "- It displays the first three rows of each dataset to provide a quick overview of the data structure and contents, allowing us to verify that the datasets have been loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(  experience  subscribe                                        hashedEmail  \\\n",
       " 0        Pro       True  f6daba428a5e19a3d47574858c13550499be23603422e6...   \n",
       " 1    Veteran       True  f3c813577c458ba0dfef80996f8f32c93b6e8af1fa9397...   \n",
       " 2    Veteran      False  b674dd7ee0d24096d1c019615ce4d12b20fcbff12d79d3...   \n",
       " \n",
       "    played_hours       name gender  age  individualId  organizationName  \n",
       " 0          30.3     Morgan   Male    9           NaN               NaN  \n",
       " 1           3.8  Christian   Male   17           NaN               NaN  \n",
       " 2           0.0      Blake   Male   17           NaN               NaN  ,\n",
       "                                          hashedEmail        start_time  \\\n",
       " 0  bfce39c89d6549f2bb94d8064d3ce69dc3d7e72b38f431...  30/06/2024 18:12   \n",
       " 1  36d9cbb4c6bc0c1a6911436d2da0d09ec625e43e6552f5...  17/06/2024 23:33   \n",
       " 2  f8f5477f5a2e53616ae37421b1c660b971192bd8ff77e3...  25/07/2024 17:34   \n",
       " \n",
       "            end_time  original_start_time  original_end_time  \n",
       " 0  30/06/2024 18:24         1.719770e+12       1.719770e+12  \n",
       " 1  17/06/2024 23:46         1.718670e+12       1.718670e+12  \n",
       " 2  25/07/2024 17:57         1.721930e+12       1.721930e+12  )"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Load the datasets\n",
    "players = pd.read_csv('data\\players.csv')\n",
    "sessions = pd.read_csv('data\\sessions.csv')\n",
    "\n",
    "# Display the first few rows of each dataset\n",
    "players.head(3), sessions.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Wrangling the Data for Further Detailed Analysis\n",
    "\n",
    "Data wrangling is a critical step in the data analysis process where we prepare the raw data for meaningful analysis. This stage involves several key tasks that ensure the data is in the appropriate format, is clean, and accurately represents the information we intend to analyze. Proper data wrangling can significantly enhance the quality of the insights derived from the data and make the analysis process smoother and more efficient.\n",
    "\n",
    "#### Objectives of Data Wrangling:\n",
    "1. Convert All Data into Proper Data Type\n",
    "\n",
    "2. Verify the Integrity of the Data\n",
    "\n",
    "3. Handling Missing Data and Dropping Unnecessary Columns \n",
    "\n",
    "4. Combine two dataframe\n",
    "\n",
    "By meticulously addressing these aspects, we can ensure that our dataset is well-structured, clean, and primed for subsequent analysis stages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-1. Convert All Data into Proper Data Type: \n",
    "\n",
    "Proper data types are essential for efficient data processing and accurate analysis. This step ensures that each column in our datasets is stored in the most appropriate format, reflecting the nature of the data and optimizing for both memory usage and processing speed. We will convert date columns to datetime objects and other relevant columns to categorical or numerical types based on their content and role in our analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experience           object\n",
      "subscribe              bool\n",
      "hashedEmail          object\n",
      "played_hours        float64\n",
      "name                 object\n",
      "gender               object\n",
      "age                   int64\n",
      "individualId        float64\n",
      "organizationName    float64\n",
      "dtype: object\n",
      "\n",
      "hashedEmail             object\n",
      "start_time              object\n",
      "end_time                object\n",
      "original_start_time    float64\n",
      "original_end_time      float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "## check the origin data type\n",
    "print(players.dtypes)\n",
    "print()\n",
    "print(sessions.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experience          category\n",
      "subscribe               bool\n",
      "hashedEmail           object\n",
      "played_hours         float64\n",
      "name                  object\n",
      "gender              category\n",
      "age                    int64\n",
      "individualId         float64\n",
      "organizationName     float64\n",
      "dtype: object\n",
      "\n",
      "hashedEmail                    object\n",
      "start_time             datetime64[ns]\n",
      "end_time               datetime64[ns]\n",
      "original_start_time           float64\n",
      "original_end_time             float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Converting data types in 'players'\n",
    "players['experience'] = players['experience'].astype('category')\n",
    "players['gender'] = players['gender'].astype('category')\n",
    "\n",
    "# Converting data types in 'sessions'\n",
    "sessions['start_time'] = pd.to_datetime(sessions['start_time'],dayfirst=True)\n",
    "sessions['end_time'] = pd.to_datetime(sessions['end_time'],dayfirst=True)\n",
    "\n",
    "## check the origin data type again\n",
    "print(players.dtypes)\n",
    "print()\n",
    "print(sessions.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By converting `experience` and `gender` in the `players` dataset to categorical types, we enhance the efficiency of our data storage and simplify the analysis involving these variables. \n",
    "\n",
    "By converting `start_time` and `end_time` from the `sessions` dataset to datetime, we enhance accurate and efficient time-based calculations. This adjustment ensures that our data handling is robust and that our analyses will be based on correctly formatted data, enabling precise and reliable results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-2. Verifying the Integrity of the Data\n",
    "\n",
    "Ensuring data integrity is a critical step before having any other data analysis. We need to check for missing values across the datasets. Missing data may significantly impact the process of the analysis, leading to biased or incorrect conclusions if not properly addressed. By identifying missing values before further analysis, we can decide on appropriate strategies for handling them, such as imputation or removal, ensuring a robust dataset for subsequent analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experience            0\n",
      "subscribe             0\n",
      "hashedEmail           0\n",
      "played_hours          0\n",
      "name                  0\n",
      "gender                0\n",
      "age                   0\n",
      "individualId        196\n",
      "organizationName    196\n",
      "dtype: int64\n",
      "\n",
      "hashedEmail            0\n",
      "start_time             0\n",
      "end_time               2\n",
      "original_start_time    0\n",
      "original_end_time      2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 2. verifying the integrity of the data\n",
    "\n",
    "## Check for missing values in each column\n",
    "missing_data_counts_players = players.isnull().sum()\n",
    "missing_data_counts_sessions = sessions.isnull().sum()\n",
    "print(missing_data_counts_players)\n",
    "print()\n",
    "print(missing_data_counts_sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By checking for missing values in each column of the `players` dataset, we can see that most columns in the `players` dataset are complete except for `individualId` and `organizationName`, which are entirely missing. This indicates that these columns may not provide any useful information for our analysis, as they contain no data at all. So we will drop them in the following wrangling.\n",
    "\n",
    "By checking for missing values in each column of the `players` dataset, we can see that the `end_time` and `original_end_time` columns each have 2 missing entries, suggesting minor issues with data recording for these specific sessions. Considering the number of missing data is small, we may fill missing `end_time` with the start_time plus the average session duration. The `original_end_time` is not needed for our future analysis and will be drop in following steps, so we can ignore it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-3. Handling Missing Data and Dropping Unnecessary Columns\n",
    "\n",
    "##### Dropping Empty Columns:\n",
    "In the `players` DataFrame, we identified two completely empty columns: `individualId` and `organizationName`. Since these columns contain no useful data, they were dropped from the DataFrame. Removing such columns is beneficial as it reduces the complexity of the data and focuses our analysis on variables that provide valuable information.\n",
    "\n",
    "##### Imputing Missing Data:\n",
    "In the `sessions` DataFrame, we noticed missing values in the `end_time` column. To address this, we calculated the average duration of all sessions and used this value to impute the missing `end_time` for sessions where it was absent. This approach assumes that the missing sessions have an average duration, which helps maintain the integrity of session length calculations across the dataset.\n",
    "\n",
    "##### Dropping Redundant Time Columns:\n",
    "We also dropped `original_start_time` and `original_end_time` from the `sessions` DataFrame. These columns were deemed unnecessary for our analysis, possibly because they were redundant or did not align with our specific analytical goals. By removing these, we simplify the dataset further, ensuring that each remaining variable has a clear purpose in our analysis.\n",
    "\n",
    "##### Verifying Data Integrity Post-Cleanup:\n",
    "After these modifications, we rechecked for missing values in both datasets to ensure that all issues were appropriately addressed. This final check helps confirm that our dataset is now clean, concise, and ready for deeper analysis, free of missing values and irrelevant columns.\n",
    "\n",
    "By performing these steps, we enhance the dataset’s usability and ensure our subsequent analyses are based on accurate and relevant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experience      0\n",
      "subscribe       0\n",
      "hashedEmail     0\n",
      "played_hours    0\n",
      "name            0\n",
      "gender          0\n",
      "age             0\n",
      "dtype: int64\n",
      "\n",
      "hashedEmail    0\n",
      "start_time     0\n",
      "end_time       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# handle missing data, drop needless columns\n",
    "\n",
    "## drop players 's empty column ['individualId', 'organizationName']\n",
    "players.drop(columns=['individualId', 'organizationName'],inplace = True)\n",
    "\n",
    "# Calculate the average duration in seconds (if time is in datetime format)\n",
    "average_duration = (sessions['end_time'] - sessions['start_time']).mean()\n",
    "\n",
    "# Iterate over the DataFrame rows using iterrows()\n",
    "for index, row in sessions.iterrows():\n",
    "    # Check if 'end_time' is NaN\n",
    "    if pd.isnull(row['end_time']):\n",
    "        # Impute missing 'end_time' by adding the average duration to 'start_time'\n",
    "        sessions.at[index, 'end_time'] = row['start_time'] + average_duration\n",
    "\n",
    "## drop sessions 's empty column ['individualId', 'organizationName']\n",
    "sessions.drop(columns=['original_start_time', 'original_end_time'],inplace = True)\n",
    "\n",
    "## Check for missing values in each column again\n",
    "missing_data_counts_players = players.isnull().sum()\n",
    "missing_data_counts_sessions = sessions.isnull().sum()\n",
    "print(missing_data_counts_players)\n",
    "print()\n",
    "print(missing_data_counts_sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-4. Combining DataFrames and Analyzing Session Durations\n",
    "\n",
    "##### Calculating Session Durations:\n",
    "Before merging, we calculate the duration of each session within the `sessions` dataframe. The duration is computed by subtracting the `start_time` from the `end_time` and converting this difference into hours. This step is essential for understanding how long players engage with the game in each session, which is a critical aspect of player behavior analysis.\n",
    "\n",
    "##### Aggregating Session Times Per Player:\n",
    "After calculating individual session durations, we aggregate these durations by player using the `hashedEmail` as a unique identifier. This aggregation provides the total session time spent by each player across all their sessions, giving us a comprehensive view of player engagement.\n",
    "\n",
    "##### Merging Session Data with Player Profiles:\n",
    "With the total session times calculated, we merge this data into the `players` dataframe. This merge operation enriches the player profiles with session duration data, allowing us to analyze player characteristics in conjunction with their gaming behavior.\n",
    "\n",
    "##### Comparing Session Times with Reported Played Hours:\n",
    "After merging, we compute the discrepancy between the reported `played_hours` in the `players` dataset and the calculated session durations. This discrepancy helps identify potential errors or anomalies in how session times are recorded or reported. It also provides insights into the accuracy of data collection methods used in the game's backend systems.\n",
    "\n",
    "##### Evaluating Data Integrity:\n",
    "The final step involves evaluating these discrepancies. By examining the differences between reported and calculated times, we can assess the reliability of our data sources and make informed decisions about further data cleaning or investigation needs.\n",
    "\n",
    "The code and these steps ensure that our analysis is built on accurate, reliable data, and provide a solid foundation for further exploratory and predictive analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experience</th>\n",
       "      <th>subscribe</th>\n",
       "      <th>hashedEmail</th>\n",
       "      <th>played_hours</th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>session_duration</th>\n",
       "      <th>discrepancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pro</td>\n",
       "      <td>True</td>\n",
       "      <td>f6daba428a5e19a3d47574858c13550499be23603422e6...</td>\n",
       "      <td>30.3</td>\n",
       "      <td>Morgan</td>\n",
       "      <td>Male</td>\n",
       "      <td>9</td>\n",
       "      <td>33.650000</td>\n",
       "      <td>-3.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Veteran</td>\n",
       "      <td>True</td>\n",
       "      <td>f3c813577c458ba0dfef80996f8f32c93b6e8af1fa9397...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Christian</td>\n",
       "      <td>Male</td>\n",
       "      <td>17</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>-0.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Veteran</td>\n",
       "      <td>False</td>\n",
       "      <td>b674dd7ee0d24096d1c019615ce4d12b20fcbff12d79d3...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Blake</td>\n",
       "      <td>Male</td>\n",
       "      <td>17</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>-0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amateur</td>\n",
       "      <td>True</td>\n",
       "      <td>23fe711e0e3b77f1da7aa221ab1192afe21648d47d2b4f...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>Flora</td>\n",
       "      <td>Female</td>\n",
       "      <td>21</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>-0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Regular</td>\n",
       "      <td>True</td>\n",
       "      <td>7dc01f10bf20671ecfccdac23812b1b415acd42c2147cb...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Kylie</td>\n",
       "      <td>Male</td>\n",
       "      <td>21</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>-0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>Amateur</td>\n",
       "      <td>True</td>\n",
       "      <td>b6e9e593b9ec51c5e335457341c324c34a2239531e1890...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Bailey</td>\n",
       "      <td>Female</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>Veteran</td>\n",
       "      <td>False</td>\n",
       "      <td>71453e425f07d10da4fa2b349c83e73ccdf0fb3312f778...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Pascal</td>\n",
       "      <td>Male</td>\n",
       "      <td>22</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>-0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>Amateur</td>\n",
       "      <td>False</td>\n",
       "      <td>d572f391d452b76ea2d7e5e53a3d38bfd7499c7399db29...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Dylan</td>\n",
       "      <td>Prefer not to say</td>\n",
       "      <td>17</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>-0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>Amateur</td>\n",
       "      <td>False</td>\n",
       "      <td>f19e136ddde68f365afc860c725ccff54307dedd13968e...</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Harlow</td>\n",
       "      <td>Male</td>\n",
       "      <td>17</td>\n",
       "      <td>2.983333</td>\n",
       "      <td>-0.683333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Pro</td>\n",
       "      <td>True</td>\n",
       "      <td>d9473710057f7d42f36570f0be83817a4eea614029ff90...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Ahmed</td>\n",
       "      <td>Other</td>\n",
       "      <td>91</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-0.050000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>196 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    experience  subscribe                                        hashedEmail  \\\n",
       "0          Pro       True  f6daba428a5e19a3d47574858c13550499be23603422e6...   \n",
       "1      Veteran       True  f3c813577c458ba0dfef80996f8f32c93b6e8af1fa9397...   \n",
       "2      Veteran      False  b674dd7ee0d24096d1c019615ce4d12b20fcbff12d79d3...   \n",
       "3      Amateur       True  23fe711e0e3b77f1da7aa221ab1192afe21648d47d2b4f...   \n",
       "4      Regular       True  7dc01f10bf20671ecfccdac23812b1b415acd42c2147cb...   \n",
       "..         ...        ...                                                ...   \n",
       "191    Amateur       True  b6e9e593b9ec51c5e335457341c324c34a2239531e1890...   \n",
       "192    Veteran      False  71453e425f07d10da4fa2b349c83e73ccdf0fb3312f778...   \n",
       "193    Amateur      False  d572f391d452b76ea2d7e5e53a3d38bfd7499c7399db29...   \n",
       "194    Amateur      False  f19e136ddde68f365afc860c725ccff54307dedd13968e...   \n",
       "195        Pro       True  d9473710057f7d42f36570f0be83817a4eea614029ff90...   \n",
       "\n",
       "     played_hours       name             gender  age  session_duration  \\\n",
       "0            30.3     Morgan               Male    9         33.650000   \n",
       "1             3.8  Christian               Male   17          4.250000   \n",
       "2             0.0      Blake               Male   17          0.083333   \n",
       "3             0.7      Flora             Female   21          0.833333   \n",
       "4             0.1      Kylie               Male   21          0.150000   \n",
       "..            ...        ...                ...  ...               ...   \n",
       "191           0.0     Bailey             Female   17               NaN   \n",
       "192           0.3     Pascal               Male   22          0.350000   \n",
       "193           0.0      Dylan  Prefer not to say   17          0.083333   \n",
       "194           2.3     Harlow               Male   17          2.983333   \n",
       "195           0.2      Ahmed              Other   91          0.250000   \n",
       "\n",
       "     discrepancy  \n",
       "0      -3.350000  \n",
       "1      -0.450000  \n",
       "2      -0.083333  \n",
       "3      -0.133333  \n",
       "4      -0.050000  \n",
       "..           ...  \n",
       "191          NaN  \n",
       "192    -0.050000  \n",
       "193    -0.083333  \n",
       "194    -0.683333  \n",
       "195    -0.050000  \n",
       "\n",
       "[196 rows x 9 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate session durations in hours\n",
    "sessions['session_duration'] = (sessions['end_time'] - sessions['start_time']).dt.total_seconds() / 3600\n",
    "\n",
    "# Aggregate total session time per player\n",
    "total_session_time = sessions.groupby('hashedEmail')['session_duration'].sum()\n",
    "\n",
    "# Merge this with player data\n",
    "players = players.merge(total_session_time, how='left', left_on='hashedEmail', right_index=True)\n",
    "\n",
    "# Compare the calculated total session time with 'played_hours'\n",
    "players['discrepancy'] = players['played_hours'] - players['session_duration']\n",
    "\n",
    "# Check discrepancies\n",
    "players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
